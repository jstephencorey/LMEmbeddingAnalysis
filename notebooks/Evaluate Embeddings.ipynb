{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/mnt/f/Dev/LMEmbeddingAnalysis/') # Change to what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from models.CustomWordEmbeddings import CustomWordEmbeddings\n",
    "from utils.utils import get_model, get_saved_embedding, clear_cuda_memory\n",
    "from utils.mteb_utils import mteb_meta, test_all_mteb\n",
    "import torch\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyis for The Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 EMBED DIM\n",
      "128 EMBED DIM\n",
      "NOW EVALUATING MODEL SentenceTransformer(\n",
      "  (0): CustomWordEmbeddings(\n",
      "    (emb_layer): Embedding(250880, 128)\n",
      "  )\n",
      "  (1): Pooling({'word_embedding_dimension': 128, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      ") /mnt/f/Dev/LMEmbeddingAnalysis/mteb_analyses/random_tok_Bloom_128_random_xn_tok_mean/\n",
      "Running task: SCIDOCS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Retrieval</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRetrieval\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - SCIDOCS, <span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">beir</span>, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2p</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "    - SCIDOCS, \u001b[3;33mbeir\u001b[0m, \u001b[3;38;5;241ms2p\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aad938e8f34c869b5478b1114a07eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859a20cbb5f941ca9540925aca622fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c1d89013234856957488f8c268a86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 EMBED DIM\n"
     ]
    }
   ],
   "source": [
    "model_tok_rand_sizes = [  128,   256,   512,   768,  1024,  1088,  1536,  2048,  2560,  4096,  5120,  7168, 9216, 12288, 14336]\n",
    "\n",
    "tokenizers = [\n",
    "                \"bigscience/tokenizer\",\n",
    "                \"cerebras/Cerebras-GPT-111M\",\n",
    "                \"facebook/opt-350m\",\n",
    "                \"google/t5-v1_1-small\",\n",
    "                'EleutherAI/gpt-neox-20b']\n",
    "\n",
    "tokenizer_nicknames = [\n",
    "    \"Bloom\",\n",
    "    \"Cerebras\",\n",
    "    \"OPT\",\n",
    "    \"T5\",\n",
    "    \"GPT-NeoX\"\n",
    "]\n",
    "\n",
    "tokenizer_embedding_sizes = {\n",
    "    \"bigscience/tokenizer\":250880,\n",
    "    \"cerebras/Cerebras-GPT-111M\":50257,\n",
    "    \"facebook/opt-350m\":50272,\n",
    "    \"google/t5-v1_1-small\":32128,\n",
    "    'EleutherAI/gpt-neox-20b':50304,\n",
    "}\n",
    "\n",
    "embedding_types = [\"random_xn_tok\"]\n",
    "\n",
    "pooling_modes = [\"mean\"] #, \"max\", \"cls\"] #Note, I had issues with these so never used them\n",
    "\n",
    "for pooling_mode in pooling_modes: \n",
    "    for embedding_type in embedding_types: \n",
    "        for tokenizer_name, tok_nick in zip(tokenizers, tokenizer_nicknames):\n",
    "            model_base_name = \"random_tok_\" + tok_nick + \"_\"\n",
    "            for model_size in model_tok_rand_sizes:\n",
    "                model_name = model_base_name + str(model_size)\n",
    "                embedding_size = (tokenizer_embedding_sizes[tokenizer_name], model_size)\n",
    "                model, output_folder = get_model(model_name, \n",
    "                                                embedding_type=embedding_type,\n",
    "                                                pooling_mode=pooling_mode,\n",
    "                                                tokenizer=tokenizer_name,\n",
    "                                                embedding_size=embedding_size)\n",
    "                if os.path.isfile(output_folder + \"mteb_metadata.md\"):\n",
    "                    print(\"Skipping!\")\n",
    "                    continue\n",
    "                print(\"NOW EVALUATING MODEL\", model, output_folder)\n",
    "\n",
    "                test_all_mteb(model, output_folder, verbosity=0)\n",
    "                clear_cuda_memory(model)\n",
    "                mteb_meta(output_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
